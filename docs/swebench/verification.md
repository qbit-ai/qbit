# Independent Verification Guide

This guide explains how to independently verify Qbit's SWE-bench evaluation results. We encourage skeptics to audit our methodology.

## What You Can Verify

1. **Dataset integrity** - Same data as official SWE-bench
2. **Patch authenticity** - Agent actually generated the fix
3. **Test execution** - Tests ran correctly in proper environment
4. **Pass/fail determination** - Correct interpretation of results

## Prerequisites

```bash
# Install SWE-bench
pip install swebench

# Ensure Docker is running
docker info

# Clone Qbit (optional, for code inspection)
git clone https://github.com/your-org/qbit
```

## Verification Steps

### Step 1: Verify Dataset Source

Confirm we use the official dataset:

```python
from datasets import load_dataset

# Load official dataset
official = load_dataset("princeton-nlp/SWE-bench_Lite", split="test")

# Load Qbit's cached dataset
import json
with open("~/.qbit/benchmarks/swebench/datasets/lite.json") as f:
    qbit_data = json.load(f)

# Compare instance count
assert len(official) == 300
assert len(qbit_data) == 300

# Compare specific instance
official_django = next(d for d in official if d['instance_id'] == 'django__django-11133')
qbit_django = next(d for d in qbit_data if d['instance_id'] == 'django__django-11133')

assert official_django['problem_statement'] == qbit_django['problem_statement']
assert official_django['FAIL_TO_PASS'] == qbit_django['fail_to_pass']
print("✓ Dataset matches official SWE-bench Lite")
```

### Step 2: Verify Agent Patch

After running an evaluation, verify the patch was generated by the agent:

```bash
# Location of evaluation results
RESULTS_DIR=./swebench-results

# Find the agent's patch
cat $RESULTS_DIR/django__django-11133/patch.diff

# Verify it's a valid git diff
git apply --check $RESULTS_DIR/django__django-11133/patch.diff

# Compare with gold patch (should be DIFFERENT)
# If identical, that would be suspicious
python -c "
from datasets import load_dataset
ds = load_dataset('princeton-nlp/SWE-bench_Lite', split='test')
instance = next(d for d in ds if d['instance_id'] == 'django__django-11133')
print('Gold patch:')
print(instance['patch'][:500])
"
```

### Step 3: Re-run Evaluation with Official Harness

The most rigorous verification: run the official SWE-bench harness on our patch.

```bash
# Create predictions file from Qbit's patch
cat > predictions.jsonl << EOF
{"instance_id": "django__django-11133", "model_name_or_path": "verification", "model_patch": "$(cat $RESULTS_DIR/django__django-11133/patch.diff | jq -Rs .)"}
EOF

# Run official harness
python -m swebench.harness.run_evaluation \
    --predictions_path predictions.jsonl \
    --swe_bench_tasks princeton-nlp/SWE-bench_Lite \
    --log_dir ./verification_logs \
    --testbed ./verification_testbed \
    --log_suffix verification \
    --timeout 600 \
    --num_processes 1

# Check results
cat ./verification_logs/*/report.json
```

### Step 4: Verify Docker Environment

Confirm we use the same Docker images as official evaluation:

```bash
# Pull the image we use
docker pull ghcr.io/epoch-research/swe-bench.eval.arm64.django__django-11133

# Compare with official
docker pull swebench/sweb.eval.django__django-11133

# Inspect environment
docker run --rm ghcr.io/epoch-research/swe-bench.eval.arm64.django__django-11133 \
    bash -c "source /opt/miniconda3/etc/profile.d/conda.sh && conda activate testbed && python --version"

docker run --rm swebench/sweb.eval.django__django-11133 \
    bash -c "source /opt/miniconda3/etc/profile.d/conda.sh && conda activate testbed && python --version"

# Should show same Python version
```

### Step 5: Verify Test Execution

Manually run the same tests the harness runs:

```bash
# Start a container
docker run -it --rm \
    -v $(pwd)/workspace:/workspace \
    ghcr.io/epoch-research/swe-bench.eval.arm64.django__django-11133 \
    bash

# Inside container:
source /opt/miniconda3/etc/profile.d/conda.sh
conda activate testbed
cd /testbed

# Apply the patch
git apply /workspace/patch.diff

# Run FAIL_TO_PASS test
./tests/runtests.py httpwrappers.tests.HttpResponseTests.test_memoryview_content

# Should PASS after applying correct fix
```

### Step 6: Verify Agent Transcript

Review the full agent interaction to confirm no cheating:

```bash
# Agent transcript location
cat $RESULTS_DIR/django__django-11133/transcript.json

# Check for suspicious patterns:
# - No references to gold patch
# - No git log/show commands
# - Logical problem-solving progression
```

## Red Flags to Look For

### Suspicious Patterns

| Pattern | Why It's Suspicious |
|---------|---------------------|
| Patch identical to gold | Agent might have seen the answer |
| Git history commands | Accessing fix commits |
| Instant correct fix | No debugging process |
| Modified test files | Gaming the evaluation |
| External API calls | Getting answers from outside |

### Verification Script

```python
#!/usr/bin/env python3
"""Verify Qbit SWE-bench results for suspicious patterns."""

import json
import sys
from pathlib import Path
from datasets import load_dataset

def verify_instance(instance_id: str, results_dir: Path):
    """Verify a single evaluation result."""
    result_path = results_dir / instance_id
    issues = []

    # Load gold patch from official dataset
    ds = load_dataset("princeton-nlp/SWE-bench_Lite", split="test")
    instance = next((d for d in ds if d['instance_id'] == instance_id), None)
    if not instance:
        return [f"Instance {instance_id} not in official dataset"]

    gold_patch = instance['patch']

    # Check 1: Patch not identical to gold
    patch_path = result_path / "patch.diff"
    if patch_path.exists():
        agent_patch = patch_path.read_text()
        if agent_patch.strip() == gold_patch.strip():
            issues.append("CRITICAL: Agent patch identical to gold patch!")
        elif gold_patch in agent_patch:
            issues.append("WARNING: Gold patch substring found in agent patch")

    # Check 2: Transcript shows legitimate process
    transcript_path = result_path / "transcript.json"
    if transcript_path.exists():
        transcript = json.loads(transcript_path.read_text())

        # Look for git history commands
        suspicious_commands = ['git log', 'git show', 'git blame', '.git/']
        for entry in transcript.get('tool_calls', []):
            for cmd in suspicious_commands:
                if cmd in str(entry):
                    issues.append(f"Suspicious command found: {cmd}")

    # Check 3: Test files not modified
    modified_files = result_path / "modified_files.txt"
    if modified_files.exists():
        for line in modified_files.read_text().splitlines():
            if 'test_' in line or '/tests/' in line:
                issues.append(f"Test file modified: {line}")

    return issues

if __name__ == "__main__":
    results_dir = Path(sys.argv[1]) if len(sys.argv) > 1 else Path("./swebench-results")

    all_issues = {}
    for instance_dir in results_dir.iterdir():
        if instance_dir.is_dir():
            issues = verify_instance(instance_dir.name, results_dir)
            if issues:
                all_issues[instance_dir.name] = issues

    if all_issues:
        print("⚠ Verification found issues:")
        for instance, issues in all_issues.items():
            print(f"\n{instance}:")
            for issue in issues:
                print(f"  - {issue}")
        sys.exit(1)
    else:
        print("✓ All instances verified successfully")
        sys.exit(0)
```

## Reproducing Results

To fully reproduce an evaluation:

```bash
# 1. Clone the repo at the same commit
git clone https://github.com/your-org/qbit
cd qbit
git checkout <evaluation-commit>

# 2. Run the same evaluation
just swebench 0-49 vertex-claude claude-opus-4-5@20251101

# 3. Compare results
diff -u original-results.json new-results.json
```

Note: LLM outputs are non-deterministic, so patches may differ while still solving the problem.

## Contacting Us

If you find issues with our evaluation:

1. **Open a GitHub issue** with detailed findings
2. **Include verification steps** you performed
3. **Attach relevant evidence** (patches, transcripts, logs)

We commit to:
- Investigating all reported concerns
- Publishing corrections if errors are found
- Updating methodology if improvements are identified

## Third-Party Audits

We welcome third-party audits. To request access to raw evaluation data:

1. Contact us via GitHub issues
2. Specify what you want to verify
3. We'll provide appropriate access

## Official SWE-bench Leaderboard

For submissions to the official leaderboard, SWE-bench maintainers perform their own verification. Our results submitted there go through their independent evaluation process.

See: https://www.swebench.com/
