# Qbit Configuration
# https://github.com/qbit-ai/qbit
#
# This file was auto-generated on first run. Customize as needed.
# Values starting with $ reference environment variables (e.g., $TAVILY_API_KEY).
# Commented-out settings show defaults - uncomment to override.

# =============================================================================
# AI Provider Configuration
# =============================================================================

[ai]
# Default provider: "vertex_ai" | "openrouter" | "anthropic" | "openai" | "ollama"
default_provider = "vertex_ai"

# Default model for the selected provider
default_model = "claude-opus-4-5@20251101"

# Default reasoning effort for models that support it (e.g., OpenAI reasoning models)
# default_reasoning_effort = "medium" # low | medium | high

# Per-sub-agent model overrides
# Use a different model for specific sub-agents (e.g., cheaper model for code generation)
# Example:
# [ai.sub_agent_models.coder]
# provider = "openai"
# model = "gpt-4o"
#
# [ai.sub_agent_models.researcher]
# provider = "vertex_ai"
# model = "claude-sonnet-4-20250514"

[ai.vertex_ai]
# Path to Google Cloud service account JSON
# credentials_path = "/path/to/service-account.json"

# Google Cloud project ID
# project_id = "your-project-id"

# Vertex AI region (default: us-east5)
# location = "us-east5"

# Show this provider's models in the model selector
# show_in_selector = true

[ai.vertex_gemini]
# Path to Google Cloud service account JSON
# credentials_path = "/path/to/service-account.json"

# Google Cloud project ID
# project_id = "your-project-id"

# Vertex AI Gemini region (default: us-central1)
# location = "us-central1"

# Show this provider's models in the model selector
# show_in_selector = true

[ai.openrouter]
# OpenRouter API key (or use $OPENROUTER_API_KEY)
# api_key = "$OPENROUTER_API_KEY"

# Show this provider's models in the model selector
# show_in_selector = true

[ai.anthropic]
# Anthropic API key for direct access (or use $ANTHROPIC_API_KEY)
# api_key = "$ANTHROPIC_API_KEY"

# Show this provider's models in the model selector
# show_in_selector = true

[ai.openai]
# OpenAI API key (or use $OPENAI_API_KEY)
# api_key = "$OPENAI_API_KEY"

# Custom base URL for OpenAI-compatible APIs
# base_url = "https://api.openai.com/v1"

# Show this provider's models in the model selector
# show_in_selector = true

[ai.ollama]
# Ollama server URL (default: http://localhost:11434)
base_url = "http://localhost:11434"

# Show this provider's models in the model selector
# show_in_selector = true

[ai.gemini]
# Google Gemini API key (or use $GOOGLE_API_KEY)
# api_key = "$GOOGLE_API_KEY"

# Show this provider's models in the model selector
# show_in_selector = true

[ai.groq]
# Groq API key (or use $GROQ_API_KEY)
# api_key = "$GROQ_API_KEY"

# Show this provider's models in the model selector
# show_in_selector = true

[ai.xai]
# xAI API key (or use $XAI_API_KEY)
# api_key = "$XAI_API_KEY"

# Show this provider's models in the model selector
# show_in_selector = true

[ai.zai_sdk]
# Z.AI native SDK configuration
# api_key = "$ZAI_API_KEY"

# Custom base URL (default: https://api.z.ai/api/paas/v4)
# Use the coding endpoint for GLM Coding Plan: https://api.z.ai/api/coding/paas/v4
# base_url = "https://api.z.ai/api/paas/v4"

# Default model
# model = "glm-4-flash"

# Show this provider's models in the model selector
# show_in_selector = true

# =============================================================================
# API Keys for Tools
# =============================================================================

[api_keys]
# Tavily API key for web search (or use $TAVILY_API_KEY)
# tavily = "$TAVILY_API_KEY"

# GitHub token for repository access (or use $GITHUB_TOKEN)
# github = "$GITHUB_TOKEN"

# =============================================================================
# User Interface Preferences
# =============================================================================

[ui]
# Theme: "dark" | "light" | "system"
theme = "dark"

# Show tips on startup
show_tips = true

# Hide banner/welcome message
hide_banner = false

# Window state is auto-saved when the window is resized or moved.
# You generally don't need to edit these manually.
[ui.window]
# width = 1400
# height = 900
# x = 100
# y = 100
# maximized = false

# =============================================================================
# Terminal Settings
# =============================================================================

[terminal]
# Default shell (if not detected from environment)
# shell = "/bin/zsh"

# Font family for terminal
font_family = "JetBrains Mono"

# Font size in pixels
font_size = 14

# Scrollback buffer lines
scrollback = 10000

# Additional commands that trigger fullterm mode (merged with built-in defaults).
# Most TUI apps (vim, htop, less) are auto-detected via ANSI sequences.
# This setting is for apps that need fullterm but don't use alternate screen buffer.
# Built-in defaults: claude, cc, codex, cdx, aider, cursor, gemini
# fullterm_commands = ["my-custom-tui", "another-app"]

# =============================================================================
# Agent Behavior
# =============================================================================

[agent]
# Session persistence: auto-save conversations
session_persistence = true

# Maximum session retention (days, 0 = forever)
session_retention_days = 30

# Enable pattern learning for tool auto-approval
pattern_learning = true

# Minimum approvals before auto-approve
min_approvals_for_auto = 3

# Approval rate threshold (0.0 - 1.0)
approval_threshold = 0.8

# =============================================================================
# MCP Servers (Model Context Protocol)
# =============================================================================

# Define MCP servers for extending agent capabilities.
# Each server has a name (table key) and connection configuration.
#
# Example: Filesystem MCP server
# [mcp_servers.filesystem]
# command = "npx"
# args = ["-y", "@anthropic-ai/mcp-server-filesystem", "/home/user/documents"]
#
# Example: GitHub MCP server
# [mcp_servers.github]
# command = "npx"
# args = ["-y", "@anthropic-ai/mcp-server-github"]
# env = { GITHUB_TOKEN = "$GITHUB_TOKEN" }
#
# Example: Custom HTTP MCP server
# [mcp_servers.custom]
# url = "http://localhost:3000/mcp"

# =============================================================================
# Trusted Repositories
# =============================================================================

# Repositories where certain actions are pre-approved.
# Uses glob patterns for matching.

[trust]
# Repositories with full trust (all tools allowed)
# full_trust = [
#     "~/Projects/personal/*",
#     "~/work/internal-tools/*",
# ]

# Repositories with read-only trust (only read tools allowed)
# read_only_trust = [
#     "~/opensource/*",
# ]

# Repositories that are never trusted (always prompt)
# never_trust = [
#     "/tmp/*",
#     "~/Downloads/*",
# ]

# =============================================================================
# Privacy & Telemetry
# =============================================================================

[privacy]
# Enable anonymous usage statistics
usage_statistics = false

# Log prompts for debugging (stored locally)
log_prompts = false

# =============================================================================
# Advanced / Experimental
# =============================================================================

[advanced]
# Enable experimental features
enable_experimental = false

# Debug logging level: "error" | "warn" | "info" | "debug" | "trace"
log_level = "info"

# Log raw LLM API request/response JSON to ./logs/api/
# Useful for debugging provider issues (e.g., random stops, malformed responses)
# enable_llm_api_logs = false

# Extract and parse the raw SSE JSON instead of logging escaped strings
# When enabled, SSE chunks are logged as parsed JSON objects instead of escaped strings
# extract_raw_sse = false

# =============================================================================
# Context Window Management
# =============================================================================

[context]
# Enable automatic context compaction when approaching context window limits.
# When enabled, old messages are automatically pruned to stay within token budget.
enabled = true

# Utilization threshold (0.0 - 1.0) at which compaction is triggered.
# At 0.80 (80%), the system will start pruning old messages.
# compaction_threshold = 0.80

# Number of recent turns (user + assistant exchanges) that are always protected
# from pruning. This ensures recent conversation context is never lost.
# protected_turns = 2

# Cooldown between compaction operations in seconds.
# Prevents excessive pruning during rapid conversation.
# cooldown_seconds = 60

# =============================================================================
# Code Indexer Settings
# =============================================================================

[indexer]
# Where to store code index files:
# - "global": Store in ~/.qbit/<codebase-name>/index (default, keeps workspace clean)
# - "local": Store in <workspace>/.qbit/index (legacy behavior)
index_location = "global"

# =============================================================================
# Telemetry & Observability
# =============================================================================

[telemetry.langfuse]
# Langfuse provides LLM observability via OpenTelemetry.
# Traces include LLM calls, tool executions, and agent workflows.
# See: https://langfuse.com/docs/integrations/opentelemetry

# Enable Langfuse tracing
enabled = false

# Langfuse host URL (defaults to https://cloud.langfuse.com)
# host = "https://cloud.langfuse.com"

# Langfuse public key (or use $LANGFUSE_PUBLIC_KEY env var)
# public_key = "$LANGFUSE_PUBLIC_KEY"

# Langfuse secret key (or use $LANGFUSE_SECRET_KEY env var)
# secret_key = "$LANGFUSE_SECRET_KEY"

# Sampling ratio for traces (0.0 to 1.0, default 1.0 = sample everything)
# Use lower values (e.g., 0.1 for 10%) for high-traffic production deployments
# sampling_ratio = 1.0

# =============================================================================
# Native OS Notifications
# =============================================================================

[notifications]
# Enable native OS notifications for agent/command completion
# Notifications are only shown when the app is not focused or the tab is inactive
# native_enabled = false

# Notification sound (macOS system sound names like "Ping" or "Blow")
# On macOS, defaults to "Blow" if not specified
# Leave blank or omit for platform default behavior
# sound = "Blow"
