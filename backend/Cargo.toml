[package]
name = "qbit"
version = "0.1.0"
description = "AI-powered terminal emulator"
authors = ["you"]
edition = "2021"
default-run = "qbit"

[lib]
name = "qbit_lib"
crate-type = ["staticlib", "cdylib", "rlib"]

[[bin]]
name = "qbit"
path = "src/main.rs"
required-features = ["tauri"]

[[bin]]
name = "qbit-cli"
path = "src/bin/qbit-cli.rs"
required-features = ["cli"]

[build-dependencies]
tauri-build = { version = "2", features = [], optional = true }

[dependencies]
# Tauri (optional - only for GUI app)
tauri = { version = "2", features = [], optional = true }
tauri-plugin-dialog = { version = "2", optional = true }

# CLI dependencies (optional - only for CLI binary)
clap = { version = "4", features = ["derive", "env"], optional = true }
atty = { version = "0.2", optional = true }

# Serialization
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Async runtime
tokio = { version = "1", features = ["full"] }

# Error handling
anyhow = "1.0"
thiserror = "1.0"

# Terminal
portable-pty = "0.8"
vte = "0.13"

# Utilities
uuid = { version = "1", features = ["v4", "serde"] }
chrono = { version = "0.4", features = ["serde"] }
dirs = "5"
parking_lot = "0.12"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
similar = "2"

# AI / LLM integration via vtcode
vtcode-core = "0.47"
vtcode-indexer = "0.47"
futures = "0.3"
dotenvy = "0.15"

# Web search
tavily = "0.2"

# Web fetching and content extraction
reqwest = { version = "0.12", features = ["json"] }
readability = "0.2"
url = "2.5"

# Anthropic on Vertex AI (custom crate)
rig-anthropic-vertex = { path = "crates/rig-anthropic-vertex" }
# Z.AI GLM with reasoning_content support
rig-zai = { path = "crates/rig-zai" }
rig-core = { version = "^0.23.1", features = [] }

# Evals framework (optional - only for evals feature)
tempfile = { version = "3", optional = true }
indicatif = { version = "0.17", optional = true }

# Google Cloud authentication (for sidecar Vertex AI synthesis)
gcp_auth = "0.12"

# Graph-based workflow execution for multi-agent systems
graph-flow = "0.4"
async-trait = "0.1"

# File walking with gitignore support
ignore = "0.4"

# Pattern matching and regex
glob = "0.3"
regex = "1"

# TOML settings file support
toml = "0.8"

# Sidecar: Local LLM inference via mistral.rs (async, auto chat templates)
# Using Metal for GPU-accelerated inference on macOS
# Optional: enable with `--features local-llm`
# TEMPORARILY DISABLED - See: https://github.com/EricLBuehler/mistral.rs/issues/XXX
# mistralrs = { git = "https://github.com/EricLBuehler/mistral.rs.git", features = ["metal"], optional = true }

# TLS crypto provider (required for rustls 0.23+)
rustls = { version = "0.23", default-features = false, features = ["ring"] }

serde_yaml = "0.9.34"

[features]
default = ["tauri", "local-tools"]
# Tauri GUI application (mutually exclusive with cli)
tauri = ["dep:tauri", "dep:tauri-build", "dep:tauri-plugin-dialog"]
# CLI binary for headless operation (mutually exclusive with tauri)
cli = ["dep:clap", "dep:atty"]
# Enable local LLM inference via mistral.rs (adds ~400MB binary size, requires Metal on macOS)
# TEMPORARILY DISABLED - mistralrs dependency commented out due to resolution conflicts
# Uncomment mistralrs dependency above and add "dep:mistralrs" to re-enable
local-llm = []
# Use local tool/session implementations instead of vtcode-core
# This enables gradual migration from vtcode-core to local implementations:
# - Default (no flag): Use vtcode-core's ToolRegistry and session_archive
# - With flag (--features local-tools): Use local implementations in src/tools/ and src/session/
local-tools = []
# Evaluation framework using rig::evals
evals = ["cli", "local-tools", "dep:tempfile", "dep:indicatif", "rig-core/experimental"]

[dev-dependencies]
tempfile = "3"
proptest = "1.4"
tower = { version = "0.5", features = ["util"] }
serial_test = "3"
